---
title: "trial experiment with PRC method"
author: "Yibin Feng"
date: '2022-04-23'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Dependencies
```{r}
rm(list = ls())

# Dependencies
library(tidyverse)
library(pencal)
#install.packages("./package/pencal_1.2.0.tar.gz", repos = NULL, type="source")
#library(splitstackshape)

print(paste("pencal version =", packageVersion("pencal")))

select <- dplyr::select

set.seed(721)

# Load wrapper function, utility function for running experiment using pencal library
source("function_pencal.R")
```

# Load data
```{r}
# Load data
# Contains two dataframes df.surv_preds and df.long_censored
load("adni_cleaned_prc.RData")

data.surv <- df.surv_preds_prc
data.long <- df.long_censored_prc
```

```{r}
cat("[Count] Number of subjects =", dim(data.surv)[1])
```

# Set up

## User define

```{r}
# Set up hyperparameters --------------------------------------------------------------------------
# Cross validation parameter
n_fold <- 10

# pencal parameters
# n.boots set to large number if CBOCP is required
n.boots <- 0
n.cores <- 1

is_ScalingYs <- FALSE # Set to TRUE to scale covariates for LMMs

# -------------------------------------------------------------------------------------------------
```


## Setting candidate covariates


```{r}
# Set candidate longitudinal covariates

# Set to FALSE to use a manually chosen set of long covairates i.e. vars_less
all_long_vars <- TRUE
# Ignore vars_less if all_long_vars is set to TRUE
vars_less <- c("ADAS13", "MMSE", "RAVLT.immediate", "RAVLT.learning", "FAQ")
# This is a subset of longitudinal covariates chosen according to previous literature

# Set cut-off for maximum proportion of subjects without any values for any covariates
missing_proportion_limit <- 0.1 # Set to 0 to consider covariates with subjects containing at least 1 measurement

# Manual exclusion ----------------------------------------------------------------------------------------
# Type of TAU, PTAU and ABETA are character, need to handle non-numerical values first. currently excluded
vars_manual_remove <- c("TAU", "PTAU", "ABETA")

# Exclude irrelevant variables
vars_irrelevant <- c(
  names(data.long)[grepl(".bl", names(data.long))], # Exclude baselines, Years.bl, Month.bl
  "RID", "time", "event", "status", "DX",
  "VISCODE", "EXAMDATE", "Y", "M", "Month",
  "AGE", "age.fup", "COLPROT", "ORIGPROT", "PTID", "SITE", 
  "PTGENDER", "PTEDUCAT", "PTETHCAT", "PTRACCAT", "PTMARRY", "APOE4",
  "FSVERSION", "IMAGEUID", "FLDSTRENG"
)
# --------------------------------------------------------------------------------------------------------


## Build model using all possible longitudinal variables

## Filter candidate longitudinal covariates automatically based on missingness criteria

missing_proportions <- data.long %>%
  select(-all_of(c(vars_irrelevant, vars_manual_remove))) %>% # Exclude manual exclusion defined above
  group_by(id) %>%
  dplyr::summarise(across(everything(), 
                          function(x) all(is.na(x)))) %>% # Within subject, all y is missing
  dplyr::select(-id) %>%
  dplyr::summarise(across(everything(),
                          function(x) sum(x) / length(unique(data.long$id)))) %>% # Proportion of subjects without any y values
  pivot_longer(cols = everything(), 
               names_to = "var_name",
               values_to = "proportion_y_missing") %>%
  arrange(proportion_y_missing) # Smaller is better

## Filter covarites with missing proportions exceeding pre-defined limit
vars_missing <- missing_proportions %>%
  filter(proportion_y_missing > missing_proportion_limit) %>% # Higher than limit is undesirable
  select(var_name) %>%
  unlist(use.names = FALSE) %>%
  sort()

# Compute union of all variables to be excluded in LMMs
vars_exclude <- Reduce(union, 
                       list(
                         "id",
                         vars_manual_remove,
                         vars_irrelevant,
                         vars_missing))
vars_selected <- names(data.long)[!(names(data.long) %in% vars_exclude)] %>%
  sort()

if (all_long_vars) {
  # Automatic selection based on criteria
  # Exclude longitudinal covariates that exceeded the missing_proportion_limit
  y.names <- vars_selected
  print("all_long_vars is set to TRUE. All available covariates will be considered and screened for missing proportions.")
  print(paste("[Report] Found", length(vars_missing), 
              "covariates with proportion of subjects without any observation exceeded user-defined limit of",
              missing_proportion_limit, "."))
  print("Inpsect `missing_proportions` dataframe to check missing proportion values.")
  print("The following covariate(s) will NOT be used for LMM: ---------------------------------------------")
  print(vars_missing)
  print("---------------------------------------------------------------------------------------------------")
} else {
  print("all_long_vars is set to FALSE. ")
  y.names <- vars_less
}

print("The following covariate will be used for LMM: -----------------------------------------------------")
print(y.names)
print("---------------------------------------------------------------------------------------------------")
print(paste("[Count] final longitudinal covariates =", length(y.names)))

# View(missing_proportions)
```

## [NOT READY TO USE] Scaling covariates
WARNING: i DID NOT use data.long_scaled below !
```{r, eval=FALSE}
# does not work yet!!! The downstream is not compatible yet!!!

# Preparation: Scaling covariates
if (is_ScalingYs) {
  data.long_scaled <- data.long # Copy data
  for (y in y.names){
    # Scaling
    data.long_scaled[, y] <- scale(data.long[, y], center = T, scale = T)
    # Uncomment to print statistics after transformation
    #cat("variable", y, "after scaling:", "\nmean =", mean(data.long_scaled[, y], na.rm = T), ", sd:", sd(data.long_scaled[, y], na.rm = T), "\n")
  }
}
```


## no CV, single split only

```{r, eval=FALSE}
# Stratified train-test split

# Train set
training.surv <- data.surv %>% 
  splitstackshape::stratified(., group = "event", size = 1 - 1 / n_fold)
training.long <- data.long %>% 
  filter(id %in% training.surv$id)
```


```{r}
# Stratified train-test split

# Train set
training.surv <- data.surv %>% 
  splitstackshape::stratified(., group = "event", size = 1 - 1 / n_fold)
training.long <- data.long %>% 
  filter(id %in% training.surv$id)

# Test set (complement to train set)
testing.surv <- data.surv %>% 
  filter(!(id %in% training.surv$id))
testing.long <- data.long %>% 
  filter(id %in% testing.surv$id)

print("Statistics after split")

# Check size
print(paste("Dataset # subjects =", dim(data.surv)[1])) # Dataset with id column
print(paste("Training set # subjects =", dim(training.surv)[1]))
print(paste("Testing set # subjects =", dim(testing.surv)[1]))

# Check class proportion
print("Dataset event proportion:")
prop.table(table(data.surv$event))
print("Training set event proportion:")
prop.table(table(training.surv$event))
print("Testing set event proportion:")
prop.table(table(testing.surv$event))

#training.long %>% group_by(id) %>% dplyr::summarise(count = n())
#testing.long %>% group_by(id) %>% dplyr::summarise(count = n())
```


## [TEMP FIX ONLY] Fix problem after splitting

```{r}
# Data problem encountered in the survpred_prclmm e.g.
# task 2 failed - "Variable ADAS13: all values are NA for at least 1 subject"

# Circumvent data problem ---------------------------------------------------------------
## Drawback is bias in performance evaluation
## select subjects with at least one value in each covariate

missing_counts <- testing.long %>%
  select(id, all_of(y.names)) %>%
  group_by(id) %>%
  dplyr::summarise(across(everything(), 
                          function(x) all(is.na(x)))) # Within subject, all y is missing

all_NA_counts <- missing_counts %>%
  select(-id) %>%
  rowSums()

valid_pred_ids <- missing_counts$id[all_NA_counts == 0]

print(paste("[Report] removed", nrow(testing.surv) - length(valid_pred_ids), 
            "subjects from original test set of size =", nrow(testing.surv)))
print(paste("[Count] number of subjects in test set with at least one observation in all longitudinal covariates =",
            length(valid_pred_ids)))

# Update testing set
testing.long.mod <- testing.long %>% 
  filter(id %in% valid_pred_ids)
testing.surv.mod <- testing.surv %>% 
  filter(id %in% valid_pred_ids)
# -----------------------------------------------------------------------------------

```


# Full model: consider all candidate covariates

## Fit model
```{r}
res <- run_prc_steps(
  long.data = training.long, 
  surv.data = training.surv, 
  y.names = y.names,
  n.boots = n.boots,
  n.cores = n.cores,
  verbose = TRUE)
```

## Model diagnostics

```{r}
# Diagnostics
ls(res$step3)
```

```{r}
res$step3$pcox.orig
```

```{r, eval=FALSE}
res$step3$pcox.orig$glmnet.fit$beta %>% 
  View()
```



## Predict survival probabilities

```{r}
# Predicted survival probabilities

# let landmark time T = 0

# a set of prediction windows
delta.T <- 1:10
```


```{r}
preds <- survpred_prclmm(
  step1 = res$step1, 
  step2 = res$step2, 
  step3 = res$step3,
  times = delta.T,
  new.longdata = testing.long.mod,
  new.basecovs = testing.surv.mod,
  keep.ranef = TRUE 
)
```

```{r}
# Inpsect predicted survival probabilities
ls(preds)
```



```{r}
# Plot predicted survival probabilities

# Organize result into df
df.preds <- preds$predicted_survival %>% 
  pivot_longer(cols = c(-id), names_to = "pred_window", values_to = "surv_prob")

df.preds$pred_window <- readr::parse_number(df.preds$pred_window)

# Plot
df.preds %>%
  ggplot() + 
    geom_point(aes(pred_window, surv_prob, group = id),
               alpha = 0.2) +
    geom_line(aes(pred_window, surv_prob, group = id),
               alpha = 0.2)

```

# [INCOMPLETE] Evaluation

warning did not set any landmark time???
```{r}
# landmark time
T.start <- 0.5

# a set of prediction windows
delta.T <- 1:10
```

## using testing set as new X and subset for landmarking
```{r}
# Dummy variables to hold the testing surv data, holding the actual surv data of test set
surv.new <- testing.surv.mod
long.new <- testing.long.mod

# Subjects at risk at landmark time
ids.at_risk <- surv.new %>% 
  filter(time > T.start) %>%
  select(id) %>%
  unlist(use.names = FALSE)

surv.new <- surv.new %>%
  filter(id %in% ids.at_risk)
long.new <- long.new %>%
  filter(id %in% ids.at_risk)
```


```{r}
# Obtain predicted random effect
# res object comes from fitted pencal
preds <- survpred_prclmm(
  step1 = res$step1, 
  step2 = res$step2, 
  step3 = res$step3,
  times = delta.T, # Prediction window(s) for surv prob prediction
  new.longdata = long.new, # Long data in test set
  new.basecovs = surv.new, # Surv data in test set
  keep.ranef = TRUE 
)

# Check dimension of predicted random effect
dim(preds$predicted_ranefs)
```




```{r}
# Obtain new X for predict on pcox.orig

# With baseline covariate, may need to expand into without baseline covariate case later
X0 <- model.matrix(as.formula(res$step3$call$baseline.covs),
                   data = surv.new)
# Join the baseline covariates and predicted random effect summary
X.orig <- as.matrix(cbind(X0, as.matrix(preds$predicted_ranefs))) 

# Drop intercept
contains.int <- "(Intercept)" %in% colnames(X.orig)
if (contains.int) {
  X.orig <- X.orig[, -1]
}

dim(X.orig)
```

## Compute td AUC
```{r}
# Compute the linear predictor
# for td AUC
linpred.orig <- predict(
  res$step3$pcox.orig, # Fitted "cv.glmnet" or "cv.relaxed" object
  newx = X.orig, # Matrix of new values for x at which predictions are to be made. Must be a matrix
  s = "lambda.min",
  type = "link" # Type "link" (default) returns x^T \beta
  )

# Compute the relative risk, exp(linear predictor)
# for concordance index
relrisk.orig <- predict(
  res$step3$pcox.orig, # Fitted "cv.glmnet" or "cv.relaxed" object
  newx = X.orig, # Matrix of new values for x at which predictions are to be made. Must be a matrix
  s = "lambda.min",
  type = "response" # Type "response" gives the ﬁtted values (mu scale)
  )
```


check whether exp(lp) is consistent, quite trivial
```{r, eval=FALSE}
data.frame(
  lp = as.numeric(linpred.orig), # linear predictor
  exp_lp = as.numeric(exp(linpred.orig)), # relative risk
  relrisk.orig = as.numeric(relrisk.orig)) %>%
  head()
```


```{r, fig.width=4}
res.tdroc <- vector(mode = "list", length = length(delta.T))
res.tdauc <- vector(mode = "numeric", length = length(delta.T))

for (i in 1:length(delta.T)) {

  predict.time <- delta.T[i]
  
  temp <- survivalROC::survivalROC(
    Stime = surv.new$time, # Event time or censoring time for subjects
    status = surv.new$event, # Indicator of status, 1 if death or event, 0 otherwise
    marker = linpred.orig, # Predictor or marker value
    entry = NULL, # Entry time for the subjects, default is NULL, why 0?
    predict.time = predict.time, # Time point of the ROC curve
    cut.values = NULL, # marker values to use as a cut-off for calculation of sensitivity and specificity
    method = "NNE", 
    span = 0.25 * nrow(surv.new)^(-0.2) # small span yield moderate smoothing, how to select?
    )
  
  res.tdauc[i] <- temp$AUC
  
  g <- ggplot(data = data.frame(TP = temp$TP, FP = temp$FP)) +
    geom_point(aes(x = FP, y = TP), size = 0.2) + 
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") + 
    labs(
      title = paste("tdROC for prediction time =", predict.time),
      subtitle = paste("tdAUC =", round(temp$AUC, 3))
    )
  
  res.tdroc[[i]] <- g
}

res.c.naive <- survcomp::concordance.index(
  x = linpred.orig, # vector of risk predictions
  surv.time = surv.new$time, # vector of event times
  surv.event = surv.new$event, # vector of event occurence indicators
  method = "noether" # conservative, noether or name (see paper Pencina et al. for details)
  )

#ls(res.c.naive)

res.c.naive$c.index
```

```{r, fig.width=15, fig.height=6}
ggpubr::ggarrange(plotlist = res.tdroc, ncol = 5, nrow = 2)
```

```{r}
data.frame(prediction.time = delta.T, 
           tdAUC = res.tdauc) %>%
  ggplot + geom_point(aes(x = prediction.time,
                          y = tdAUC)) + 
  coord_cartesian(ylim = c(0, 1))
```


alternatively, use survcomp wrapper for survivalROC to construct td ROC curves
don't know why the AUC become different...
is it due to the x i'm using?
```{r}
predict.time <- 1

res.tdauc.naive <- survcomp::tdrocc(
  x = relrisk.orig, # vector of exp(lp)
  surv.time = surv.new$time,
  surv.event = surv.new$event,
  time = predict.time, # time point for the ROC curve
  span = 0.25 * nrow(surv.new)^(-0.2)
)

g <- ggplot(data = data.frame(TP = res.tdauc.naive$sens,
                         FP = 1 - res.tdauc.naive$spec)) +
  geom_point(aes(x = FP, y = TP), size = 0.2) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") + 
  labs(
    title = paste("tdROC for prediction time =", predict.time),
    subtitle = paste("AUC =", round(res.tdauc.naive$AUC, 3))
  )

print(g)
```

```{r}
predict.time <- 1

res.tdauc.naive <- survcomp::tdrocc(
  x = linpred.orig, # vector of lp
  surv.time = surv.new$time,
  surv.event = surv.new$event,
  time = predict.time, # time point for the ROC curve
  span = 0.25 * nrow(surv.new)^(-0.2)
)

g <- ggplot(data = data.frame(TP = res.tdauc.naive$sens,
                         FP = 1 - res.tdauc.naive$spec)) +
  geom_point(aes(x = FP, y = TP), size = 0.2) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") + 
  labs(
    title = paste("tdROC for prediction time =", predict.time),
    subtitle = paste("AUC =", round(res.tdauc.naive$AUC, 3))
  )

print(g)
```




# [To be removed]

## getting descriptive statistics about observations per subject on each candidate covariate
```{r, fig.width=12}
count_non_na <- function(x) {
  sum(!is.na(x))
  } # Count number of non-NA values in a vector

# Construct df for each covariate to inspect frequency of non-NA values per subject
res.count_non_na <- data.long %>% 
  group_by(id) %>% # Per subject
  dplyr::summarize(across(all_of(vars_selected), count_non_na)) # Count non-NA


# Plot histogram for each covariate for visual inspection
plots <- lapply(y.names, function(y){
  res.count_non_na %>% ggplot() + 
    stat_count(aes(get(y))) +
    coord_cartesian(xlim = c(0, 20)) +
    labs(title = y) + 
    xlab("# obs per subject") + ylab("Frequency")
})

# Combine plot
n_panel <- 6 # How many subplots in one combined plot
ids.notplot <- 1:length(plots)
while(length(ids.notplot) > 0){
  ids <- head(ids.notplot, n_panel)
  p <- ggpubr::ggarrange(plotlist = plots[ids]) # Combine plot
  print(p)
  ids.notplot <- ids.notplot[!(ids.notplot %in% ids)]
}
rm(ids.notplot, n_panel, ids, p)
```




## [wip] Generate a intersection of subject ids with at least one observation in each covariate
```{r}

keep_ids <- lapply(y.names, function(y){
  temp.long <- subset(training.long, !is.na(training.long[ , y]))
  return(unique(temp.long$id))
})

Reduce(intersect, keep_ids) %>% length()
```



# Applying pencal to covariates with at least one observation for each subject i.e. easier to work with at this moment

this section is probably no more needed after fixing bugs and checking.


```{r}
# Any subject without observation
y_any_sub_no_obs <- res.count_non_na %>% dplyr::summarise(across(all_of(y.names), function(x) any(x==0)))

# Apply PRC without any filtering step, now works for longitudinal covariates that all subjects have at least 1 observation
y.names_subset <- y.names[y_any_sub_no_obs == FALSE]

cat("Longitudinal covariates in which each subject consists of at least 1 observation include: \n", y.names_subset, "\n")

res <- run_prc_steps(
  long.data = training.long, 
  surv.data = training.surv, 
  y.names = y.names_subset,
  n.boots = n.boots, 
  n.cores = n.cores,
  verbose = TRUE)
```

There are 13 longitudinal covariates in this model. Runs successfully.

```{r}
length(y.names_subset)
y.names_subset
```





alternative version wip

```{r}
# Any subject without observation
missing_proportions

# Apply PRC without any filtering step, now works for longitudinal covariates that all subjects have at least 1 observation
y.names_subset <- y.names[y_any_sub_no_obs == FALSE]

cat("Longitudinal covariates in which each subject consists of at least 1 observation include: \n", y.names_subset, "\n")

res <- run_prc_steps(
  long.data = training.long, 
  surv.data = training.surv, 
  y.names = y.names_subset,
  n.boots = n.boots, 
  n.cores = n.cores,
  verbose = TRUE)
```









# [NOT REPORTED] Univariate (longitudinal) model
For debug purpose

```{r, eval=FALSE}
candidate_long_vars <- missing_proportions %>% 
  filter(var_name %in% y.names)

# Fit univariate (longitudinal) model for each candidate long covariate to diagnose problem
## Initiate result column
candidate_long_vars$runtime <- NA

## Run pencal 
candidate_long_vars$runtime <- sapply(y.names, function(y){
  y.names_subset <- as.vector(y)
  cat("\n>>>\nFitting long covariate:", y.names_subset, "\n")
  try(run_prc_steps(
    long.data = training.long, 
    surv.data = training.surv, 
    y.names = y.names_subset, 
    n.boots = n.boots, 
    n.cores = n.cores), silent = FALSE)
})
```

```{r, eval=FALSE}
# Result table
## Does it run? (Y)-with total runtime in mins; (N)-with error message
candidate_long_vars
#View(candidate_long_vars)
```




