---
title: "Untitled"
author: "Yibin Feng"
date: '2022-06-09'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
source("function_utility_exp.R")
source("function_evaluation.R")

source("function_glmnet_exp.R")

```


# Load data
refer to folds template prepared in run_all_exp.rmd

```{r}
rm("folds")
rm("folds.eval")

print(path.template)
load(file = path.template)

# Model param
is_scaled <- "scaled" # Set to "scaled" to scale covariates for LMMs
is_transformed <- is_transformed
landmark <- paste0("lm", T.start)

hyperparam <- paste(c(landmark, is_transformed, is_scaled), collapse = "_") # Use hyperparam to describe model

print(hyperparam)
```


# Train

At this stage, folds list for cross validation should be initiated.

```{r}
# -----------------------------------------------------------------------------------
# Fit models for each fold
# -----------------------------------------------------------------------------------
for (i in 1:n_fold) {
  print("---------------------------------------------------------------------------------------------------")
  print(paste("Start training in fold", i))
  print("---------------------------------------------------------------------------------------------------")
  # -----------------------------------------------------------------------------------
  # General - Subset subjects for fold i
  # -----------------------------------------------------------------------------------
  tmp <- Get_train_test_data(
    data.surv = data.surv, 
    data.long = data.long,  
    ids.test = folds[[i]]$ids.test, 
    is_scaled = is_scaled, 
    scaling_table = folds[[i]]$scaling_table
  )
  
  training.surv <- tmp$training.surv
  training.long <- tmp$training.long
  
  # -----------------------------------------------------------------------------------
  # pCox - train model
  # -----------------------------------------------------------------------------------
  # Baseline covariates used for fitting penalized Cox model in all folds
  baseline.covs <- folds[[i]]$baseline.covs
  baseline.covs.additional <- folds[[i]]$candidate.long.covs
  
  scenario <- folds[[i]]$scenario # Deciding number candidate covariates used

  # Note: imputation inside
  res <- Train_glmnet(
    training.surv = training.surv,
    training.long = training.long,
    scenario = scenario,
    baseline.covs = baseline.covs,
    baseline.covs.additional = baseline.covs.additional
  )
  
  # -----------------------------------------------------------------------------------
  # Store results
  # -----------------------------------------------------------------------------------
  folds[[i]]$glmnet <- list(
    cvfit = res$cvfit,
    covs.pcox = res$covs.pcox,
    runtimes = res$runtimes,
    is_scaled = is_scaled
  )
  # -----------------------------------------------------------------------------------
}
# -----------------------------------------------------------------------------------
```




# Test


```{r}
folds.eval <- vector(mode = "list", length = n_fold)

for (i in 1:n_fold) {
  # -----------------------------------------------------------------------------------
  # General - Subset subjects for fold i
  # -----------------------------------------------------------------------------------
  tmp <- Get_train_test_data(
    data.surv, data.long, 
    folds[[i]]$ids.test,
    is_scaled, 
    folds[[i]]$scaling_table
    )
  
  surv.new <- tmp$testing.surv
  long.new <- tmp$testing.long
  
  # testing.surv <- tmp$testing.surv
  # testing.long <- tmp$testing.long
  # ----------------------------------------------------------------------------------- 
  # Landmarking -only consider subjects at risk at landmark time T.start
#   ids.at_risk <- testing.surv %>% 
#     filter(time > T.start) %>%
#     select(id) %>%
#     unlist(use.names = FALSE)
#   
#   surv.new <- testing.surv %>%
#     filter(id %in% ids.at_risk)
#   
#   # Commented out this part to use all longitudinal information
# #  long.new <- testing.long %>%
# #    filter(id %in% ids.at_risk) %>%
# #    filter(Years.bl <= T.start) # Filter long observations after landmark time
#   long.new <- testing.long %>%
#     filter(id %in% ids.at_risk)
  
#  print(paste("[Count] subject at risk at landmark", length(ids.at_risk)))
  
  # -----------------------------------------------------------------------------------  
  # pCox - Evaluate model
  # ----------------------------------------------------------------------------------- 
  # Step 1 - prepare new X
  # Subset columns to construct model matrix on new data
  covs.pcox <- folds[[i]]$glmnet$covs.pcox
  testing.x.covs <- surv.new %>%
    select(all_of(covs.pcox))
  # Mean imputation
  testing.x.covs.imputed <- Imputate.x.mean(testing.x.covs)
  # Model matrix, of dimension n obs x n vars; each row is an observation vector
  testing.x.mat <- model.matrix(~ ., data = testing.x.covs.imputed)
  # Observed response
  testing.y <- survival::Surv(
    time = surv.new$time,
    event = surv.new$event,
    type = "right"
  )
  # Step 2 - compute linear predictor
  linpred <- predict(
    folds[[i]]$glmnet$cvfit, # Fitted "cv.glmnet" object
    newx = testing.x.mat, # Matrix of new values for x at which predictions are to be made. Must be a matrix
    s = "lambda.min",
    type = "link" # Type "link" (default) returns x^T \beta
    )
  
  # -----------------------------------------------------------------------------------
  # General - Compute tdROC and tdAUC, c-index
  # -----------------------------------------------------------------------------------
  res.tdauc <- Evaluate_tdauc(surv.new, linpred, T.start, deltaT)
  res.c.naive <- survcomp::concordance.index(
    x = linpred, # vector of risk predictions
    surv.time = surv.new$time, # vector of event times
    surv.event = surv.new$event, # vector of event occurence indicators
    method = "noether" # conservative, noether or name (see paper Pencina et al. for details)
    )
  # -----------------------------------------------------------------------------------
  
  # -----------------------------------------------------------------------------------
  # Store results
  # -----------------------------------------------------------------------------------
#  folds.eval[[i]]$ids.test_no_missing <- ids.valid
  folds.eval[[i]]$perf <- list(
    landmark = T.start,
    deltaT = deltaT,
    c.index = res.c.naive$c.index,
    tdauc = res.tdauc$tdauc,
    tp = res.tdauc$tp,
    fp = res.tdauc$fp
  )
  # -----------------------------------------------------------------------------------
}
```


# Inpsect results

```{r}
print("Print c-index in CV")
cv.c.index <- sapply(folds.eval, function(x) x$perf$c.index)
mean(cv.c.index) %>% round(3)
sd(cv.c.index) %>% round(3)
```

```{r}
# Estimated tdAUC for deltaT
cv.tdauc <- data.frame(
  deltaT = folds.eval[[1]]$perf$deltaT,
  tdauc.fold = sapply(folds.eval, function(x) x$perf$tdauc) # row is fold, column is prediction window
)

# Plot figure for average tdauc in single CV
cv.tdauc %>%
  mutate(mean = rowMeans(across(starts_with("tdauc")))) %>%
  select(deltaT, mean) %>%
  round(3) %>%
  ggplot() + 
    geom_point(aes(x = deltaT, y = mean)) +
    geom_line(aes(x = deltaT, y = mean)) +
    scale_y_continuous(breaks = 0:10 / 10) +
    coord_cartesian(ylim = c(0, 1)) +
    xlab("Time since baseline") + ylab("cv tdAUC")

```


```{r, fig.width=15, fig.height=9, eval=FALSE}
g <- Plot_all_tdROC(folds.eval)

g
```

# Save


```{r}
# Save evaluation result
tmp <- paste0("./output/eval_", set_scenario, "_pCox_", hyperparam, "_seed", seed, ".RData")
save(folds.eval, file = tmp)

print(paste("Performance in folds saved to path:", tmp))
```

```{r}
# [Optional] save model
tmp <- paste0("./model/model_", set_scenario, "_pCox_", is_scaled, "_seed", seed, ".RData")
save(folds, file = tmp)

print(paste("Trainined models in folds saved to path:", tmp))
```

